{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Firstname Lastname](https://) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/).\n",
    "\n",
    "For questions/comments/improvements, email author@email.address.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `Introduction to spaCy` `1`\n",
    "\n",
    "This is lesson `1` of 3 in the educational series on `Natural Language Processing (NLP) with spaCy`. This notebook is intended `to teach the basics of NLP and the spaCy library.`. \n",
    "\n",
    "**Audience:** `Teachers` / `Learners` / `Researchers`\n",
    "\n",
    "**Use case:** `Tutorial`\n",
    "\n",
    "`A tutorial is a carefully constructed example that takes the user by the hand through a series of steps to learn how a process works. Tutorials often use \"toy\" (or at least carefully constrained) examples that give reliable, accurate, and repeatable results every time.`\n",
    "\n",
    "**Difficulty:** `Beginner`\n",
    "\n",
    "`Beginner assumes users are relatively new to Python and Jupyter Notebooks. The user is helped step-by-step with lots of explanatory text.`\n",
    "\n",
    "`Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.`\n",
    "\n",
    "`Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the process being explained.`\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "```\n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "```\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "```\n",
    "* Basic file operations (open, close, read, write)\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "```\n",
    "1. Understand what NLP is generally\n",
    "2. Understand the basics of spaCy\n",
    "3. Understand Containers\n",
    "4. Understand the Doc object\n",
    "5. Understand the Token and its Attributes\n",
    "```\n",
    "**Research Pipeline:**\n",
    "```\n",
    "N/A\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "`List out any libraries used and what they are used for`\n",
    "* [spaCy](https://spacy.io/) for performing [Natural Language Processing (NLP)](https://docs.constellate.org/key-terms/#nlp).\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\wma22\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (62.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 11.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.3.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (62.1.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "# Using !pip installs\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "# Using %%bash magic with apt-get and yes prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction to the Notebooks\n",
    "\n",
    "In these notebooks, students will receive an introduction to natural language processing, or NLP, and the Python library spaCy. SpaCy allows users to perform NLP tasks via Python. In academia, the Natural Language Toolkit, or NLTK, has remained quite popular. SpaCy is an industry alternative to NLTK.\n",
    "\n",
    "I enjoy using spaCy in all my NLP workflows for a few reasons. First, spaCy's syntax, or the way in which you write code to do things with the spaCy library, is fairly straightforward. Second, it allows you to construct robust pipelines for processing texts and extracting relevant information from them. Third, spaCy has powerful built-in components that allow you to use both heuristics, or rules, and machine learning. Fourth, out-of-the-box, spaCy offers pipelines and machine learning components for many modern languages. Each month their team and the community add more support for existing languages and support new languages. Fifth, training new machine learning models via spaCy is relatively simple and easy to replicate. Sixth, spaCy has a strong community and forum; if you have a question, someone is usually their to help. Seventh, spaCy scales well, meaning you can process many texts efficiently. Eighth, it is comprehensive enough to add new languages into its framework.\n",
    "\n",
    "In these three notebooks, students will learn the basics of spaCy and how to use it to solve a real-world problem in a digital humanities setting, namely information extraction via rules-based named entity recognition. Because these notebooks are designed as a primer for students new to NLP and spaCy, there are many subjects left out of these notebooks, mainly machine learning and how to train custom machine learning spaCy models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c98f19",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part One: The Basics of NLP and spaCy\n",
    "\n",
    "In this notebook, we will not be working with spaCy in code, rather in concept. This entire JupyterBook is designed around approaching spaCy top-down. By this I mean approaching the things that spaCy does and can do and then exploring how to implement that in code. I think this is necessary so that as you explore the smaller components of spaCy, such as the Lemmatizer, you will understand how it fits into the larger architecture of the spaCy framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b18bc5-eb89-4132-8aa0-b7affe9e92f6",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407335a3-cff6-4deb-a96d-557abae42112",
   "metadata": {},
   "source": [
    "## What is spaCy?\n",
    "\n",
    "A good way to begin is by exploring the question, \"What is spaCy?\" spaCy (yes, spelled with a lowercase \"s\" and uppercase \"C\" is a natural language processing framework. **Natural language processing**, or NLP, is a branch of linguistics that seeks to parse human language in a computer system. This field is generally referred to as computational linguistics, though it has far reaching applications beyond academic linguistic research.\n",
    "\n",
    "NLP is used in every sector of industry, from academics who leverage it to aid in research to financial analysts who try and predict the stock market. Lawyers use NLP to help analyze thousands of legal documents in seconds to target their research and medical doctors use it to parse patient charts. NLP has been around for decades, but with the increased promise of deep learning, a subfield of machine learning, that NLP rapidly expanded. This is because, as we shall learn all too well throughout this book, language is inherently ambiguous. By this, I mean that language does not always make perfect sense. In some cases, it is entirely illogical. The double-negative in English is a good example of this. In some contexts, it can be an emphatic positive, as in, \"I cannot stress this enough, I do not like pasta.\" This is, of course a lie. I love pasta, but you get my point. In other cases, the double negative can be an emphatic negative, as in, \"I ain't not doing that!\"\n",
    "\n",
    "As humans, especially native speakers of a language, we can parse these complex illogical statements with ease, especially with enough context. For computers, this is not always easy.\n",
    "\n",
    "Because NLP is such a complex problem for computers, it requires a complex solution. The answer has been found in artificial neural networks, or ANNs or neural nets for short. These are the primary areas of research for deep learning practitioners. As the field of deep learning (and machine learning in general) expand and advance, so too does NLP. New methods for training, such as transformer models, push the field further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc764ba-bae9-45ba-841a-c8d27b989f4f",
   "metadata": {},
   "source": [
    "## How to Install spaCy\n",
    "\n",
    "In order to install spaCy, I recommend visiting their website, here: https://spacy.io/usage. They have a nice user-friendly interface. Input your device settings, e.g. Mac or Windows or Linux, and your language, e.g. English, French, or German. The web-app will automatically populate the commands that you need to execute to get started. Since this is a JupyterBook, we can install these with a \"!\" before in a cell to indicate that we want to run a terminal command. I will be installing spaCy and thee small English model, en_core_web_sm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd88eaf0-9f3b-4b83-a7b9-197862a3860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f019c5-ea06-4c84-a812-b5ef5dfc7b3e",
   "metadata": {},
   "source": [
    "As with all Python libraries, the first thing we need to do is import spaCy. In the last notebook, I walked you through how to install it and download the small English model. If you have followed those steps, you should be able to import it like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c72720",
   "metadata": {},
   "source": [
    "## spaCy's Quickstart Page\n",
    "\n",
    "On spaCy's page, you can find a helpful [quickstart guide](https://spacy.io/models#quickstart). This is the best reference available with up-to-date information for how to get started with each language officially supported by spaCy.\n",
    "\n",
    "![spaCy's Quickstart page](\"../images/spacy_quickstart.JPG\")\n",
    "\n",
    "On this page, you will notice that you have a few choices to select. The dropdown menu lets you select language. The current supported languages are:\n",
    "\n",
    "    - Catalan\n",
    "    - Chinese\n",
    "    - Croatian\n",
    "    - Danish\n",
    "    - Dutch\n",
    "    - English\n",
    "    - Finnish\n",
    "    - French\n",
    "    - German\n",
    "    - Greek\n",
    "    - Italian\n",
    "    - Japanese\n",
    "    - Korean\n",
    "    - Lithuanian\n",
    "    - Macedonian\n",
    "    - Multi-language\n",
    "    - Norwegian Bokmål\n",
    "    - Polish\n",
    "    - Portuguese\n",
    "    - Romanian\n",
    "    - Russian\n",
    "    - Slovenian\n",
    "    - Spanish\n",
    "    - Swedish\n",
    "    - Ukrainian\n",
    "\n",
    "\n",
    "Next, you can select \"Loading style\", this is how you intend to load a model in a Python script. A packaged spaCy pipeline functions like a Python library, or module. This means you can load it like any other Python package. While useful for some applications, most documentation you will see will use `spaCy.load()`, where we import spaCy into our script and load a pipeline with `load()`.\n",
    "\n",
    "This page then asks you to select for either efficiency or accuracy. Efficiency refers to computational efficiency, meaning a smaller pipeline that can run faster. Accuracy refers to a model that is substantially larger, slower, and far more accurate. By default, the efficiency pipeline is the small pipeline and the accuracy pipeline is the transformer pipeline (discussed below).\n",
    "\n",
    "Finally, you have a checkbox for `show text example`. This will provide you with a text example. On some browsers, this checkbox does not do anything and it is selected by default.\n",
    "\n",
    "\n",
    "## Naming Conventions for spaCy Pipelines\n",
    "\n",
    "Official spaCy pipelines have standard naming conventions. They look like this: `en_core_web_sm` or `en_core_web_trf`. Let's break down what this means.\n",
    "\n",
    "- `en`: Language. This indicates the language of the model. These are the standard 2-character representation for languages.\n",
    "- `core`: Type. this indicates the type of model or its capabilities. Core indicates that this is a general pipeline that can recognize all required spaCy components, such as tagger, parser, lemmatizer, and named entity recognition.\n",
    "- `web`: Genre. This indicates the type of data that wasa used for training. You will typically see `web` and `news` for most languages.\n",
    "- `sm`: Size. There are four sizes for spaCy pipelines: small (sm), medium (md), large (lg), and transformer (trf).\n",
    "\n",
    "Each model from spaCy is also versioned, meaning it is meant to work alongside a specific version of spaCy. As spaCy updates its core library, it also retrains its models. You will receive error messages if you use older versions of the models indicating that they not be fully supported. This does not mean that you have to update every time spaCy puts out a new version (about once every other month). Most updates are minor and do not effect the core languages significantly. However, you should know about this as some updates do create changes that can break workflows.\n",
    "\n",
    "A key version issue to be aware of is the change from spaCy 2x (version 2) to 3x (version 3). This was a substantial overhaul, especially in how models were trained. We will learn about this in Week 3 of this course. The key thing to be aware of now is that the documentation that you are using is for spaCy 3x and the model you are using is for 3x. If you ever want to make sure of a model's current version, you can use pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2ba1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: en-core-web-sm\n",
      "Version: 3.4.1\n",
      "Summary: English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
      "Home-page: https://explosion.ai\n",
      "Author: Explosion\n",
      "Author-email: contact@explosion.ai\n",
      "License: MIT\n",
      "Location: c:\\users\\wma22\\anaconda3\\lib\\site-packages\n",
      "Requires: spacy\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip show en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6cf5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c44134c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576da751-1ca9-4632-a1ea-c140a660fe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wma22\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\wma22\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\wma22\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "c:\\Users\\wma22\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdcbc87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f85f29-f885-49b6-bb2a-5efe1bc4c33f",
   "metadata": {},
   "source": [
    "With spaCy imported, we can now create our nlp object. This is the standard Pythonic way to create your model in a Python script. Unless you are working with multiple models in a script, try to always name your model, nlp. It will make your script much easier to read. To do this, we will use spacy.load(). This command tells spaCy to load up a model. In order to know which model to load, it needs a string argument that corresponds to the model name. Since we will be working with the small English model, we will use \"en_core_web_sm\". This function can take keyword arguments to identify which parts of the model you want to load, but we will get to that later. For now, we want to import the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e189bc2b-169d-481b-a420-26a9553abf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362fb53-72f4-4cfc-85ba-c7e1144753ec",
   "metadata": {},
   "source": [
    "## Why use spaCy? A Quick Example (IN-CLASS LESSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b252ab-c19f-4aaa-89b1-4ab4e8e2fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68f34a2e-21ad-4ff2-914e-f4c13c704e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“To Thomas Jefferson from George Wythe, 9 March 1770,” Founders Online, National Archives, https://founders.archives.gov/documents/Jefferson/01-01-02-0027. [Original source: The Papers of Thomas Jefferson, vol. 1, 1760–1776, ed. Julian P. Boyd. Princeton: Princeton University Press, 1950, p. 38.]\n",
      "I send you some nectarine and apricot graffs and grapevines, the best I had; and have directed your messenger to call upon Major Taliaferro for some of his. You will also receive two of Foulis’s catalogues. Mrs. Wythe will send you some garden peas.\n",
      "You bear your misfortune so becomingly, that, as I am convinced you will surmount the difficulties it has plunged you into, so I foresee you will hereafter reap advantages from it several ways. Durate, et vosmet rebus servate secundis.\n"
     ]
    }
   ],
   "source": [
    "with open (\"../data/fo_jefferson.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0a3928-544c-4fae-ac88-3989d599a40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I send you some nectarine and apricot graffs and grapevines, the best I had; and have directed your messenger to call upon Major Taliaferro for some of his. You will also receive two of Foulis’s catalogues. Mrs. Wythe will send you some garden peas.\n",
      "You bear your misfortune so becomingly, that, as I am convinced you will surmount the difficulties it has plunged you into, so I foresee you will hereafter reap advantages from it several ways. Durate, et vosmet rebus servate secundis.\n"
     ]
    }
   ],
   "source": [
    "text = data.splitlines()[1:]\n",
    "text = \"\\n\".join(text)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f8b07d-7939-499d-ad5c-234412965328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I send you some nectarine and apricot graffs and grapevines, the best I had; and have directed your messenger to call upon Major Taliaferro for some of his. You will also receive \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Foulis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       "’s catalogues. Mrs. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wythe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " will send you some garden peas.</br>You bear your misfortune so becomingly, that, as I am convinced you will surmount the difficulties it has plunged you into, so I foresee you will hereafter reap advantages from it several ways. Durate, et vosmet rebus servate secundis.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd250c3-aea1-495d-906b-1f8302fa3833",
   "metadata": {},
   "source": [
    "## Why Master spaCy? (IN-CLASS LESSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a6ea17-fab4-43e7-af4a-5cadcf24226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I send you some nectarine and apricot graffs and grapevines, the best I had; and have directed your messenger to call upon \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Major Taliaferro\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for some of his. You will also receive \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Foulis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       "’s catalogues. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mrs. Wythe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " will send you some garden peas.</br>You bear your misfortune so becomingly, that, as I am convinced you will surmount the difficulties it has plunged you into, so I foresee you will hereafter reap advantages from it several ways. Durate, et vosmet rebus servate secundis.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.util import filter_spans\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "import re\n",
    "salutation_names_pattern = r\"(Mrs|Major)(\\.)* [A-Z]\\w+( [A-Z]\\w+)*\"\n",
    "@Language.component(\"salutation_person\")\n",
    "def salutation_person(doc):\n",
    "    text = doc.text\n",
    "    person_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(salutation_names_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        person_ents.append((span.start, span.end, span.text))\n",
    "    for start, end, name in person_ents:\n",
    "        per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)\n",
    "nlp.add_pipe(\"salutation_person\", before=\"ner\")\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848cdf6-528b-42ae-b1ba-66ee9f5826ea",
   "metadata": {},
   "source": [
    "# Part Two: Getting Started with spaCy and its Linguistic Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed78c2-880b-4016-8b43-238d96edb9cd",
   "metadata": {},
   "source": [
    "In this part of the notebook, we will start working with spaCy directly. The goals of this chapter are twofold. First, it is my hope that you understand the basic spaCy syntax for creating a Doc container and how to call specific attributes of that container. Second, it is my hope that you leave this chapter with a basic understanding of the vast linguistic annotations available in spaCy. While we will not explore all attributes, we will deal with many of the most important ones, such as lemmas, parts-of-speech, and named entities. By the time you are finished with this chapter, you should have enough of a basic understanding of spaCy to begin applying it to your own texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27962c-734e-416f-accd-c85a4eeed71d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36584ab3-2fa8-44aa-9e9d-322f52df5ae8",
   "metadata": {},
   "source": [
    "The first thing new spaCy students need to understand is the hierarchy of spaCy data objects. In spaCy, this means beginning to interact with and understand containers. **`Containers are spaCy objects that contain a large quantity of data about a text.`** When we analyze texts with the spaCy framework, we create different container objects to do that. Here is a full list of all spaCy containers. We will be focusing on three (emboldened): Doc, Span, and Token.\n",
    "\n",
    "* <b>Doc</b>\n",
    "* DocBin\n",
    "* Example\n",
    "* Language\n",
    "* Lexeme\n",
    "* <b>Span</b>\n",
    "* SpanGroup\n",
    "* <b>Token</b>\n",
    "\n",
    "I created the image below to show how I visualize spaCy containers in my mind. At the top, we have a Doc container. This is the basis for all spaCy. It is the main object that we create. Within the Doc container are many different attributes and subcontainers. One attribute is the Doc.sents, which contains all the sentences in the Doc container. The doc container (and each sentence generator) is made up of a set of token containers. These are things like words, punctuation, etc.\n",
    "\n",
    "Span containers are kind of like token, in that they are a piece of a Doc container. Spans have one thing that makes them unique. They can cross multiple tokens.\n",
    "\n",
    "We can give spans a bit more specificity by classifying them into different groups. These are known as SpanGroup containers.\n",
    "\n",
    "\n",
    "</center><img src=\"http://spacy.pythonhumanities.com/_images/spacy_containers.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3537c15e-7eb5-4b39-98ee-cab1e53861d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../data/wiki_us.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a87115-8df3-4807-9044-df3fc8c64552",
   "metadata": {},
   "source": [
    "Now, let's see what this text looks like. It can be a bit difficult to read in a JupyterBook, but notice the horizontal slider below. You don't neeed to read this in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f3143d-e4d5-489c-a4b2-4af55be81d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775â€“1783), which established independence. In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The Spanishâ€“American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II.\n",
      "\n",
      "During the Cold War, the United States fought the Korean War and the Vietnam War but avoided direct military conflict with the Soviet Union. The two superpowers competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.\n",
      "\n",
      "The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations. It is a permanent member of the United Nations Security Council. Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.\n",
      "\n",
      "The United States is a highly developed country, accounts for approximately a quarter of global GDP, and is the world's largest economy. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.2% of the world's total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]\n"
     ]
    }
   ],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc206e7a-d51e-4fca-86f4-2b8a27be82a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a Doc Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52573e88-f481-496d-ac88-37a7d1b2cccd",
   "metadata": {},
   "source": [
    "With the data loaded in, it's time to make our first Doc container. Unless you are working with multiple Doc containers, it is best practice to always call this object \"doc\", all lowercase. To create a doc container, we will usually just call our nlp object and pass our text to it as a single argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a897b4-b13b-41a8-a89b-d31bd6597568",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dd677-d872-486f-bea2-ad575335135d",
   "metadata": {},
   "source": [
    "Great! Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f62254b-9cc7-45b4-9696-69090848e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775â€“1783), which established independence. In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The Spanishâ€“American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II.\n",
      "\n",
      "During the Cold War, the United States fought the Korean War and the Vietnam War but avoided direct military conflict with the Soviet Union. The two superpowers competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.\n",
      "\n",
      "The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations. It is a permanent member of the United Nations Security Council. Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.\n",
      "\n",
      "The United States is a highly developed country, accounts for approximately a quarter of global GDP, and is the world's largest economy. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.2% of the world's total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]\n"
     ]
    }
   ],
   "source": [
    "print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5567f9-0cfb-4d4f-922a-684cf50775f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652\n",
      "3525\n"
     ]
    }
   ],
   "source": [
    "print (len(doc))\n",
    "print (len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f408baf-04f3-4e48-b922-4aed9ba8d501",
   "metadata": {},
   "source": [
    "Hmm... What's going on here? Same text, but different length. Why does this occur? To answer that, let's explore it more deeply and try and print off each item in each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a637cac-a3c4-4086-9141-ae3c544bcf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "U\n",
      "n\n",
      "i\n",
      "t\n",
      "e\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "for token in text[:10]:\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb64b4bf-193d-407f-9345-d83aa2317cfe",
   "metadata": {},
   "source": [
    "As we would expect. We have printed off each character, including white spaces. Let's try and do the same with the Doc container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81758a1a-33be-44d6-8c15-afb75dd9a234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "United\n",
      "States\n",
      "of\n",
      "America\n",
      "(\n",
      "U.S.A.\n",
      "or\n",
      "USA\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:10]:\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5cdd3-5a6c-4ac7-abc2-c87386dd665f",
   "metadata": {},
   "source": [
    "And now we see the magical difference. While on the surface it may seem that the Doc container's length is dependent on the quantity of words, look more closely. You should notice that the open and close parentheses are also considered an item in the container. These are all known as tokens. **Tokens** are a fundamental building block of spaCy or any NLP framework. They can be words or punctuation marks. Tokens are something that has syntactic purpose in a sentence and is self-contained. A good example of this is the contraction \"don't\" in English. When tokenized, or the process of converting the text into tokens, we will have two tokens. \"do\" and \"n't\" because the contraction represents two words, \"do\" and \"not\".\n",
    "\n",
    "On the surface, this may not seem exceptional. But it is. You may be thinking to yourself that you could easily use the split method in Python to split by whitespace and have the same result. But you'd be wrong. Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51a3c3fd-7930-4ba4-be67-0671b51ef8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "United\n",
      "States\n",
      "of\n",
      "America\n",
      "(U.S.A.\n",
      "or\n",
      "USA),\n",
      "commonly\n",
      "known\n"
     ]
    }
   ],
   "source": [
    "for token in text.split()[:10]:\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7efbbf-f6c7-468d-8172-27848c204f61",
   "metadata": {},
   "source": [
    "Notice that the parentheses are not removed or handled individually. To see this more clearly, let's print off all tokens from index 5 to 8 in both the text and doc objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "661c832a-18a7-47bf-ad12-a605045700f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8f1197f-c3a3-4ab4-9d44-2feffb217971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy Token 5:\n",
      "(\n",
      "Word Split 5:\n",
      "(U.S.A.\n",
      "\n",
      "\n",
      "SpaCy Token 6:\n",
      "U.S.A.\n",
      "Word Split 6:\n",
      "or\n",
      "\n",
      "\n",
      "SpaCy Token 7:\n",
      "or\n",
      "Word Split 7:\n",
      "USA),\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=5\n",
    "for token in doc[i:8]:\n",
    "    print (f\"SpaCy Token {i}:\\n{token}\\nWord Split {i}:\\n{words[i]}\\n\\n\")\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb6779-e9e2-4d28-af31-85f38f66d3bb",
   "metadata": {},
   "source": [
    "We can see clearly now how the spaCy Doc container does much more with its tokenization than a simple split method. We could, surely, write complex rules for a language to achieve the same results, but why bother? SpaCy does it exceptionally well for all languages. In my entire time using spaCy, I have never seen the tokenizer make a mistake. I am sure that mistakes may occur, but these are probably rare exceptions.\n",
    "\n",
    "Let's see what else this Doc Container holds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cf622-a453-409d-b667-b1fb7b42f3c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentence Boundary Detection (SBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0b915-3aee-4f2e-9635-c2285fa2558a",
   "metadata": {},
   "source": [
    "In NLP, sentence boundary detection, or SBD, is the identification of sentences in a text. Again, this may seem fairly easy to do with rules. One could use split(\".\"), but in English we use the period to also denote abbreviation. You could, again, write rules to look for periods not proceeded by a lowercase word, but again, I ask the question, \"why bother?\". We can use spaCy and in seconds have all sentences fully separated through SBD.\n",
    "\n",
    "To access the sentences in the Doc container, we can use the attribute sents, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a0fc02a-9454-40f7-bd55-a51311bdc6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n",
      "It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j]\n",
      "At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d]\n",
      "The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22]\n",
      "With a population of more than 331 million people, it is the third most populous country in the world.\n",
      "The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "\n",
      "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century.\n",
      "The United States emerged from the thirteen British colonies established along the East Coast.\n",
      "Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775â€“1783), which established independence.\n",
      "In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent.\n",
      "Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition.\n",
      "The Spanishâ€“American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II.\n",
      "\n",
      "\n",
      "During the Cold War, the United States fought the Korean War and the Vietnam War but avoided direct military conflict with the Soviet Union.\n",
      "The two superpowers competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon.\n",
      "The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.\n",
      "\n",
      "\n",
      "\n",
      "The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature.\n",
      "It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations.\n",
      "It is a permanent member of the United Nations Security Council.\n",
      "Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration.\n",
      "The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption.\n",
      "However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.\n",
      "\n",
      "\n",
      "The United States is a highly developed country, accounts for approximately a quarter of global GDP, and is the world's largest economy.\n",
      "By value, the United States is the world's largest importer and the second-largest exporter of goods.\n",
      "Although its population is only 4.2% of the world's total, it holds 29.4% of the total wealth in the world, the largest share held by any country.\n",
      "Making up more than a third of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b6345-2ce2-43fe-930e-52c5bc3e0b43",
   "metadata": {},
   "source": [
    "Let's move forward with just one of these sentences. Let's try and grab index 0 in this attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93f1dd9f-16aa-48c4-871f-98d40a3f8566",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentence1 \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (sentence1)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "sentence1 = doc.sents[0]\n",
    "print (sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d53e2a-e684-4254-b9b2-047ad854c310",
   "metadata": {},
   "source": [
    "Uh oh! We got an error. That is because the sents attribute is a generator. It is beyond the scope of this notebook to explain what generators are or how they work. Instead, let's convert our genreator into a list so that we can work with it by each index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea60d54e-3685-452d-83c7-abd68a798185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n"
     ]
    }
   ],
   "source": [
    "sentence1 = list(doc.sents)[0]\n",
    "print (sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1109fb5-88d7-4a8a-9c0e-1c10cfb0e092",
   "metadata": {},
   "source": [
    "Now we have the first sentence. Now that we have a smaller text, let's explore spaCy's other building block, the token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20666e3a-15a7-4953-b5c4-9e6c7f6b316c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Token Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80317dd9-c592-4ec6-9ae5-f8c16eb1474d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The token object contains a lot of different attributes that are VITAL do performing NLP in spaCy. We will be working with a few of them, such as:\n",
    "\n",
    "* .text\n",
    "* .head\n",
    "* .left_edge\n",
    "* .right_edge\n",
    "* .ent_type_\n",
    "* .iob_\n",
    "* .lemma_\n",
    "* .morph\n",
    "* .pos_\n",
    "* .dep_\n",
    "* .lang_\n",
    "\n",
    "I will briefly describe these here and show you how to grab each one and what they look like. We will be exploring each of these attributes more deeply in this chapter and future chapters. To demonstrate each of these attributes, we will use one token, \"States\" which is part of a sequence of tokens that make up \"The United States of America\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e3443-2915-427f-b611-8027020038f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2 = sentence1[2]\n",
    "print (token2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706a597-8893-42c3-9684-181a48170ec9",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3722f-5982-4fa8-9a14-40dcada1d04e",
   "metadata": {},
   "source": [
    "```Verbatim text content.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890389a-5732-471f-ad65-422a0e95c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ba31c-cef2-4d6c-9fa8-d88f546b4030",
   "metadata": {},
   "source": [
    "### Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e18c98-ca54-48ee-8a0c-aa2736c53cc6",
   "metadata": {},
   "source": [
    "```The syntactic parent, or “governor”, of this token.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa475622-2589-48ba-bb8c-11dc1bbf59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298977d-8dea-4e99-8fbd-cefd1bd2e0a5",
   "metadata": {},
   "source": [
    "This tells to which word it is governed by, in this case, the primary verb, \"is\", as it is part of the noun subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0519a7f-4789-4592-8889-7512405a1560",
   "metadata": {},
   "source": [
    "### Left Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41b5cb-f842-4137-975e-9422ae533ad9",
   "metadata": {},
   "source": [
    "``` The leftmost token of this token’s syntactic descendants.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85683d16-64f4-40dd-b3a5-3ad801572820",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.left_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2df84-d59e-4215-a7e6-eb18830e1b02",
   "metadata": {},
   "source": [
    "If part of a sequence of tokens that are collectively meaningful, known as **multi-word tokens**, this will tell us where the multi-word token begins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e8dfa-84e9-409e-8d0d-220333762277",
   "metadata": {},
   "source": [
    "### Right Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0b184-6f47-4895-8b64-8faa557df83e",
   "metadata": {},
   "source": [
    "``` The rightmost token of this token’s syntactic descendants.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ab833-33e4-47c8-a20f-b07573562081",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.right_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c6706-5977-4d91-8bfc-7397e57e9461",
   "metadata": {},
   "source": [
    "This will tell us where the multi-word token ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea6d9b-d853-466f-b0ed-ed0810faa1c0",
   "metadata": {},
   "source": [
    "### Entity Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a36a8-aa64-438e-be7c-4ec2bdd950a9",
   "metadata": {},
   "source": [
    "``` Named entity type.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d968c4-d27a-45a6-b5f5-68ec6a1f4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.ent_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168ab7c-12c9-4f33-b77f-5015de27af0f",
   "metadata": {},
   "source": [
    "Note the absence of the _ at the end of the attribute. This will return an integer that corresponds to an entity type, where as _ will give you the string equivalent., as in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952a63b-d91f-4d0c-b488-4003fe17b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.ent_type_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb77eb-b6e9-42ca-973b-1cbac310e8dc",
   "metadata": {},
   "source": [
    "We will learn all about types of entities in our chapter on named entity recognition, or NER. For now, simply understand that GPE is geopolitical entity and is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350393fb-c458-485e-99f5-25a61ffa9138",
   "metadata": {},
   "source": [
    "### Ent IOB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c5275-e685-458f-86b4-0a95db1de7d9",
   "metadata": {},
   "source": [
    "```IOB code of named entity tag. “B” means the token begins an entity, “I” means it is inside an entity, “O” means it is outside an entity, and \"\" means no entity tag is set.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9d6a9-795b-4c09-bdae-7240834de910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token2.ent_iob_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff3e58-054b-4155-9d81-7e41add6c642",
   "metadata": {},
   "source": [
    "IOB is a method of annotating a text. In this case, we see \"I\" because states is inside an entity, that is to say that it is part of the United States of America."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60acbf-f39c-4486-ac5d-0f71a65814dd",
   "metadata": {},
   "source": [
    "### Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860c1ca-b67d-48ee-b627-e7d6f9663c84",
   "metadata": {},
   "source": [
    "```Base form of the token, with no inflectional suffixes.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee687a5-ddd5-4e8e-96bf-31528414f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a675d-d97c-447a-8241-79dc579b11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1[12].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8dacd-9b02-4bf5-af92-a2be8da32e26",
   "metadata": {},
   "source": [
    "### Morph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927abe78-ad00-40d0-9d78-63ecc8a47e99",
   "metadata": {},
   "source": [
    "```Morphological analysis``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93388e18-5623-44ac-b31e-06d7841df218",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1[12].morph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a3018-329e-4994-b427-cd8f78cf18a8",
   "metadata": {},
   "source": [
    "### Part of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b25725-e15a-4efc-baac-cbeac379b246",
   "metadata": {},
   "source": [
    "```Coarse-grained part-of-speech from the Universal POS tag set.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56981e-ed57-4d28-9f88-3baeb883cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce86f8-05c5-477f-a977-df2b815e529d",
   "metadata": {},
   "source": [
    "### Syntactic Dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520dc111-d0be-479e-8fca-92871de34251",
   "metadata": {},
   "source": [
    "```Syntactic dependency relation.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff783dc0-b1a9-446b-83a5-d05b9e53c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.dep_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1ccec-94ad-4d7a-9696-0b14303c12a2",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7c739-5a7f-43d6-a8d5-6f98a919ae93",
   "metadata": {},
   "source": [
    "```Language of the parent document’s vocabulary.``` -spaCy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4dfdd-779b-47d4-b151-4970a85cc981",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.lang_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2db8e8-01e1-4c07-b22f-786394565883",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8adee-f003-432b-93f5-c6b970c02b8d",
   "metadata": {},
   "source": [
    "In the field of computational linguistics, understanding parts-of-speech is essential. SpaCy offers an easy way to parse a text and identify its parts of speech. Below, we will iterate across each token (word or punctuation) in the text and identify its part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e07df3-32e9-45c0-ae69-3496230b7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sentence1:\n",
    "    print (token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812c056-5478-4453-92f7-983e7c681bd6",
   "metadata": {},
   "source": [
    "Here, we can see two vital pieces of information: the string and the corresponding part-of-speech (pos). For a complete list of the pos labels, see the spaCy documentation (https://spacy.io/api/annotation#pos-tagging). Most of these, however, should be apparent, i.e. PROPN is proper noun, AUX is an auxiliary verb, ADJ, is adjective, etc. We can visualize this sentence with a diagram through spaCy's displaCy Notebook feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f12ac-c6dc-4fd8-9ad5-ebc8e51b0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae196b38-a489-4ef3-a269-77c61686f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(sentence1, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bc79-1f3b-4495-957d-833c1b9c2bd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee60636-4c64-4f0e-9adf-42bdfdf2f705",
   "metadata": {},
   "source": [
    "Another essential task of NLP, is named entity recognition, or NER. I spoke about NER in the last notebook. Here, I’d like to demonstrate how to perform basic NER via spaCy. Again, we will iterate over the doc object as we did above, but instead of iterating over doc.sents, we will iterate over doc.ents. For our purposes right now, I simply want to print off each entity’s text (the string itself) and its corresponding label (note the _ after label). I will be explaining this process in much greater detail in the next two notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbf328-94d4-4491-a1ea-0429d4be0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4281b-4bdc-4ea6-805b-ebe3abd5c3ae",
   "metadata": {},
   "source": [
    "Sometimes it can be difficult to read this output as raw data. In this case, we can again leverage spaCy's displaCy feature. Notice that this time we are altering the keyword argument, style, with the string \"ent\". This tells displaCy to display the text as NER annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38cc08-720f-4e6e-9b2f-54b7778fd403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "`I know we covered a lot in this notebook and the best way to understand its contents in depth is to apply it to your own domain, or area of expertise. I encourage you to select a text (or texts) that you use in your own research and try to apply the methods covered in this notebook to those particular texts. I would highly encourage you to do this before moving on to the next notebook.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a6d92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c7fd8c",
   "metadata": {},
   "source": [
    "1. <a id=\"1\"></a> Here is an anchor link footnote.\n",
    "2. <a id=\"2\"></a> D'Ignazio, Catherine and Lauren F. Klein. [*Data Feminism*](https://mitpress.mit.edu/books/data-feminism). MIT Press, 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
